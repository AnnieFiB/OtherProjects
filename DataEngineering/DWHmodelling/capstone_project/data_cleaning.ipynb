{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1e3395-7d97-4e60-9a7e-514fd7a0ea57",
   "metadata": {},
   "source": [
    "# Capstone Project: Sales Intelligence Platform for Strategic Retail Decisions\n",
    "<p align=\"center\"><strong>Author: Anthonia, Specialization: Analysis,Business Focus: Sales, Tool: Panda, PostgreSQL</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4d8df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf59ce9-43e7-496b-931a-0add9f2438fe",
   "metadata": {},
   "source": [
    "- Scope:\n",
    "> Build a centralized analytics platform for a nationwide retailer using SQL, Python, and PostgreSQL. The project spans OLTP/OLAP modeling, data cleaning, ETL, and advanced analysis.\n",
    "- Problem Statement:\n",
    "> Sales insights are fragmented across spreadsheets; thereâ€™s no unified data warehouse, making profitability tracking and strategic decisions difficult.\n",
    "- Objectives:\n",
    "> - Normalise raw sales data into a clean OLTP schema with surrogate keys.\n",
    "> - Design an OLAP data warehouse for dimensional analysis (BI).\n",
    "> - Automate data cleaning, transformation, and database loading.\n",
    "> - Enable advanced SQL-based reporting and insights using SQL and BI tools.\n",
    "- Key Deliverables:\n",
    "> - OLTP & OLAP schema creation (with ERD and SK mappings) --schema.sql / ERD.png\n",
    "> - Cleaned DataFrames and transformation logic -- data_cleaning.ipynb\n",
    "> - PostgreSQL loading script -- load_to_db.py\n",
    "> - Advanced SQL query file (CTEs, window functions, aggregations) -- advanced_sql_queries.sql\n",
    "> - cleaned_csvs/ â€“ Output datasets (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a50784-045b-47c8-bbdf-41a2672b24f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Project  Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22d70-d43e-4107-bfb8-38c58070a9b1",
   "metadata": {},
   "source": [
    "1. Data Modeling (OLTP + OLAP)\n",
    ">- Define normalized tables (Orders, Customers, Products, Payments,Locations)\n",
    ">-  Design dimensional model (FactSales: holds metrics like Amount, Profit, Quantity; DimCustomer; DimProduct; DimDate; DimLocation; DimPayment\n",
    ">- \n",
    " Generate ERD and SQL schema scrip\n",
    "\n",
    "2. Data Cleaning & Transformation\n",
    ">-  Clean raw CSVs using Panda\n",
    ">- \n",
    " Generate cleaned tables: customer_sales_df, product_sales_df, et\n",
    ">- \n",
    " Assign surrogate keys and export to \n",
    "\n",
    "3. ETL & Database Integration\n",
    ">-  Load cleaned data into PostgreSQ\n",
    ">- \n",
    " Support inserts, updates, and auto-refre\n",
    ">- \n",
    " Use SQLAlchemy for integrat\n",
    "\n",
    "4. Analytical Queries & Reporting\n",
    ">-  Join OLAP tables for insight\n",
    ">- \n",
    " Use CTEs, window functions, CASE, and aggregatio\n",
    ">- Generate monthly KPIs and percentile filters\n",
    ">- Track profit and loss by category, city, and customer.Measure order frequency, high-profit sub-categories, and peak months.\n",
    "-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ba64e-479f-40c7-8b36-b37776c95a21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddab56c-2a73-47cb-8d12-d90a9c4e8a9b",
   "metadata": {},
   "source": [
    "![datamodel](datamodel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63571272-47bd-4f39-a5e9-e3590db1adb7",
   "metadata": {},
   "source": [
    "## Load library & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfff8996-be0c-40fa-b007-74c454a583fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment ready!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext sql\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Image, display\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from data_cleaning import *\n",
    "\n",
    "\n",
    "print(\"âœ… Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a5e10-9e12-4bf1-8605-6df3136ac4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_df =pd.read_csv(\"sales_historical_data.csv\")\n",
    "display(sales_df.head(3))\n",
    "display(sales_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eaa40f-0c4d-4310-a5ec-5edec30eab45",
   "metadata": {},
   "source": [
    "### Understanding dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc50adb-2a13-4958-a2e0-f2537e78ac69",
   "metadata": {},
   "source": [
    "| **Column Name**   | **Description**                                                                 |\n",
    "|-------------------|----------------------------------------------------------------------------------|\n",
    "| Order ID          |Identifier for each order line.                                               |\n",
    "| Amount            | **Total sale value** of the order (includes both cost and profit).              |\n",
    "| Profit            | Profit earned from the order (i.e., `Amount - Cost`).         \n",
    "| Quantity          | Number of items sold in the order.                                              |\n",
    "| Category          | Broad classification of the product (e.g., Electronics).                        |\n",
    "| Sub-Category      | Specific type of product within the category (e.g., Printers, Electronic Games).|\n",
    "| PaymentMode       | Payment method used (e.g., UPI, Credit Card).                                   |\n",
    "| Order Date        | Date when the order was placed.                                                 |\n",
    "| CustomerName      | Name of the customer who placed the order.                                      |\n",
    "| State , City            | State where the order was delivered.                                            |\n",
    "\n",
    "Order ID isnt unique (same ID is assigned to different customers) . This will be renamed to **orderlineID** and a new surrogate key used as the primary key to prevent failure into db table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace9aa6-f80e-4c6b-94a7-7f0802ec09da",
   "metadata": {},
   "source": [
    "## Data-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394bd2f-7661-47e0-a425-8e1d97fc94f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df = sales_df.copy()\n",
    "print(f\"âœ… Loaded {len(cleaned_df)} rows and {len(cleaned_df.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f5418-fb21-4c93-91a8-c74cf5774cdd",
   "metadata": {},
   "source": [
    "### Step 2: Standardize column names to snake_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b064e93-a42e-4474-8ddc-7a62ab3fcc61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\" Step 2: Standardizing column names...\")\n",
    "cleaned_df = standardize_column_names(cleaned_df)\n",
    "print(f\"âœ… Columns after cleaning: {list(cleaned_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6bbb4-bb96-452e-982a-ca6db4528161",
   "metadata": {},
   "source": [
    "### Step 3: Handle missing values in critical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775aec6b-b1ba-4bd4-b3fb-127653a7ed15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "critical_cols = [\"order_id\"]\n",
    "print(f\" Step 3: No missing rows with nulls in {critical_cols}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7521623-3b0d-40e2-8c6f-f1d5dc49fb9d",
   "metadata": {},
   "source": [
    "### Step 4: Convert date fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0af778-0b78-4d49-b564-9cce6d38d33f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\" Step 4: Converting 'order_date' and year_month'  to datetime...\")\n",
    "cleaned_df[\"order_date\"] = pd.to_datetime(cleaned_df[\"order_date\"], errors=\"coerce\")\n",
    "print(f\"âœ… Converted 'order_date' to datetime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffaca3-1986-4e9d-949d-ceabb9a464bb",
   "metadata": {},
   "source": [
    "### Step 5: Remove full duplicates (across all columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d776be3-2707-469f-afa9-a6ca2a95e81f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\" Step 5: Removing full duplicate rows (based on all columns)...\")\n",
    "before = len(cleaned_df)\n",
    "cleaned_df = cleaned_df.drop_duplicates()\n",
    "print(f\"âœ… Removed {before - len(cleaned_df)} full duplicate rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bab385-e4d7-4a49-bb09-8c5e364d1f20",
   "metadata": {},
   "source": [
    "### Step 6: Standardize object columns (text cleanup) with non-ID categorical fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a1faf-227b-497a-b28c-0557774517c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Step 6: Standardizing categorical fields (excluding ID columns)...\")\n",
    "categorical_cols = [\n",
    "    col for col in cleaned_df.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "    if \"id\" not in col.lower()\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    cleaned_df[col] = cleaned_df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "print(f\"âœ… Standardized fields: {categorical_cols}\")\n",
    "\n",
    "exclude_cols = [\"customer_name\"]\n",
    "filtered_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "for col in filtered_cols:\n",
    "    print(f\"\\nðŸ”Ž Unique values in '{col}':\")\n",
    "    print(cleaned_df[col].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d99b12-b6cc-491c-860c-b5f9e57aa5f4",
   "metadata": {},
   "source": [
    "### Step 7: Rename 'order_id' to 'orderline_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260d527-2957-48a4-84bb-600e52e33af4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df.rename(columns={\"order_id\": \"orderline_id\"}, inplace=True)\n",
    "print(\" Step 7: order_id column renamed to 'orderline_id' \")\n",
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69086e-92ad-49d5-b340-596626e6fee1",
   "metadata": {},
   "source": [
    "## Table Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1259bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create oltp tables with PK and SK\n",
    "customers_df = cleaned_df[['customer_name', 'city', 'state']].drop_duplicates().reset_index(drop=True)\n",
    "customers_df['customer_id'] = customers_df.index + 1\n",
    "customers_df['customer_sk'] = customers_df['customer_id']\n",
    "\n",
    "products_df = cleaned_df[['category', 'sub_category']].drop_duplicates().reset_index(drop=True)\n",
    "products_df['product_id'] = products_df.index + 1\n",
    "products_df['product_sk'] = products_df['product_id']\n",
    "\n",
    "payments_df = cleaned_df[['payment_mode']].drop_duplicates().reset_index(drop=True)\n",
    "payments_df['payment_id'] = payments_df.index + 1\n",
    "payments_df['payment_sk'] = payments_df['payment_id']\n",
    "\n",
    "locations_df = cleaned_df[['city', 'state']].drop_duplicates().reset_index(drop=True)\n",
    "locations_df['location_id'] = locations_df.index + 1\n",
    "locations_df['location_sk'] = locations_df['location_id']\n",
    "\n",
    "# Merge to build orders_sales_df with foreign keys\n",
    "orders_df = cleaned_df.merge(customers_df, on=['customer_name', 'city', 'state']) \\\n",
    "              .merge(products_df, on=['category', 'sub_category']) \\\n",
    "              .merge(payments_df, on='payment_mode') \\\n",
    "              .merge(locations_df, on=['city', 'state'])\n",
    "\n",
    "# Add primary key and surrogate key\n",
    "orders_df = orders_df.reset_index(drop=True)\n",
    "orders_df['order_id'] = orders_df.index + 1  # PK\n",
    "orders_df['order_sk'] = orders_df['order_id']  # SK (same as PK for now)\n",
    "\n",
    "# Final fact table (OLTP-level)\n",
    "orders_df = orders_df[[\n",
    "    'order_id', 'order_sk', 'orderline_id', 'order_date', 'year_month',\n",
    "    'customer_sk', 'product_sk', 'payment_sk', 'location_sk',\n",
    "    'amount', 'profit', 'quantity'\n",
    "]]\n",
    "\n",
    "display(orders_df.head(3))\n",
    "display(customers_df.head(3))       \n",
    "display(products_df.head(3))\n",
    "display(payments_df.head(3))\n",
    "display(locations_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88747cf2",
   "metadata": {},
   "source": [
    "### Build OLAP dimension tables using SKs from OLTP tables (De-normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84123b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DimCustomer\n",
    "dim_customer = customers_df[['customer_sk', 'customer_name']].copy()\n",
    "\n",
    "# DimProduct\n",
    "dim_product = products_df[['product_sk', 'category', 'sub_category']].copy()\n",
    "\n",
    "# DimPayment\n",
    "dim_payment = payments_df[['payment_sk', 'payment_mode']].copy()\n",
    "\n",
    "# DimLocation\n",
    "dim_location = locations_df[['location_sk', 'city', 'state']].copy()\n",
    "\n",
    "# DimDate\n",
    "dim_date = orders_df[['order_date']].drop_duplicates().reset_index(drop=True)\n",
    "dim_date['date_id'] = dim_date.index + 1\n",
    "dim_date['day'] = dim_date['order_date'].dt.day\n",
    "dim_date['month'] = dim_date['order_date'].dt.month\n",
    "dim_date['quarter'] = dim_date['order_date'].dt.quarter\n",
    "dim_date['year'] = dim_date['order_date'].dt.year\n",
    "dim_date['day_of_week'] = dim_date['order_date'].dt.day_name()\n",
    "\n",
    "# FactSales table\n",
    "fact_sales = orders_df.merge(dim_date, on='order_date', how='left')\n",
    "\n",
    "fact_sales = fact_sales[[\n",
    "    'order_sk', 'customer_sk', 'product_sk', 'payment_sk', 'location_sk',\n",
    "    'date_id', 'quantity', 'amount', 'profit'\n",
    "]]\n",
    "\n",
    "display(dim_customer.head(3))\n",
    "display(dim_product.head(3))        \n",
    "display(dim_payment.head(3))\n",
    "display(dim_location.head(3))\n",
    "display(dim_date.head(3))\n",
    "display(fact_sales.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62079a-72d3-4663-b130-a5ee62ec12ce",
   "metadata": {},
   "source": [
    "## Save cleaned df to file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994400dc-0bcb-4a5d-896a-862f824bcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_df.to_csv(\"customers.csv\", index=False)\n",
    "#products_df.to_csv(\"products.csv\", index=False)\n",
    "#locations_df.to_csv(\"locations.csv\", index=False)\n",
    "#payments_df.to_csv(\"payments.csv\", index=False)\n",
    "#orders_df.to_csv(\"orders.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e92be-c0d7-4f94-a1f2-effbf2161d98",
   "metadata": {},
   "source": [
    "## Create OLTP Schema and Load Cleaned Data into pgAdmin with Error Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcda966-aa04-42d5-b265-a29b455cf251",
   "metadata": {},
   "source": [
    "###  Step 1: Check if DB (sales_db) exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2476116",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "        'dbname': os.getenv('SALES_DB_NAME', 'postgres'),\n",
    "        'user': os.getenv('SALES_DB_USER', 'postgres'),\n",
    "        'password': os.getenv('SALES_DB_PASSWORD', ''),\n",
    "        'host': os.getenv('SALES_DB_HOST', 'localhost'),\n",
    "        'port': os.getenv('SALES_DB_PORT', '5432')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d74b8e-2e43-4a09-b714-0b85711f6188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Database 'sales_db' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Check, create if needed, and connect\n",
    "check_and_create_db(\n",
    "    target_dbname=\"sales_db\",\n",
    "    url_env_var=\"BASE_URL\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb26ef-00e1-437d-a624-88699f3e6131",
   "metadata": {},
   "source": [
    "###  Step 2: Connect to PostgreSQL db (sales_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a490f84-bd11-4cb7-b659-74ca1dd0d989",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_db_connection] âœ… Connected using URL from 'SALES_DB_URL'\n"
     ]
    }
   ],
   "source": [
    "#conn, curr = get_db_connection(env_prefix=\"SALES_DB_\")  # uses default prefix DB_\n",
    "conn, cur = get_db_connection(url_var=\"SALES_DB_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2774a1-ec20-4514-9fd7-ad218b7d5f19",
   "metadata": {},
   "source": [
    "### Step 3 Schema + Table Creation with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"schema.sql\", \"r\") as f:\n",
    "        curr.execute(f.read())\n",
    "    conn.commit()\n",
    "    print(\"âœ… Schema created successfully.\")\n",
    "\n",
    "    # Run additional queries with the same cursor\n",
    "    curr.execute(\"SELECT * FROM information_schema.tables WHERE table_schema = 'oltp'\")\n",
    "    oltp_tables = curr.fetchall()\n",
    "    \n",
    "    curr.execute(\"SELECT * FROM information_schema.tables WHERE table_schema = 'olap'\")\n",
    "    olap_tables = curr.fetchall()\n",
    "\n",
    "    print(\"OLTP Tables:\", [t[2] for t in oltp_tables])\n",
    "    print(\"OLAP Tables:\", [t[2] for t in olap_tables])\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error creating schema:\", e)\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e41d6",
   "metadata": {},
   "source": [
    "### ERD Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"erd.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f7f30-a6ea-46db-a7a8-9055e0065632",
   "metadata": {},
   "source": [
    "### Step 4: Load Data Using Copy_Expert from Memory with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661bff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "        upsert_from_df(conn, customers_df, 'customers', ['customer_id'], schema='oltp')\n",
    "        upsert_from_df(conn, products_df, 'products', ['product_id'], schema='oltp')\n",
    "        upsert_from_df(conn, payments_df, 'payments', ['payment_id'], schema='oltp')\n",
    "        upsert_from_df(conn, locations_df, 'locations', ['location_id'], schema='oltp')\n",
    "        upsert_from_df(conn, orders_df, 'orders', ['order_id'], schema='oltp')\n",
    "\n",
    "        upsert_from_df(conn, dim_customer, 'dim_customer', ['customer_sk'], schema='olap')\n",
    "        upsert_from_df(conn, dim_product, 'dim_product', ['product_sk'], schema='olap')\n",
    "        upsert_from_df(conn, dim_payment, 'dim_payment', ['payment_sk'], schema='olap')\n",
    "        upsert_from_df(conn, dim_location, 'dim_location', ['location_sk'], schema='olap')\n",
    "        upsert_from_df(conn, dim_date, 'dim_date', ['date_id'], schema='olap')\n",
    "        upsert_from_df(conn, fact_sales, 'fact_sales', ['order_sk'], schema='olap')\n",
    "\n",
    "        print(\" All tables successfully uploaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"âŒ Error during table uploads:\", e)\n",
    "        conn.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1484a-a4b1-47fb-84f7-2fecaa4cbe9e",
   "metadata": {},
   "source": [
    "### Step 5: Verify DB Load Successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd2a60-815d-4bd2-beeb-579c78beff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = \"SELECT * FROM olap.fact_sales LIMIT 10;\"\n",
    "    fact_df = pd.read_sql_query(query, conn)\n",
    "    print(\"âœ… Query executed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to execute query: {e}\")\n",
    "fact_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222b61f-6e74-4d71-afdf-d6bb0f303af9",
   "metadata": {},
   "source": [
    "###  Step 6: Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b8632-18e3-4cea-883e-c09e011554bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "curr.close()\n",
    "conn.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c197e7-f58d-4aa2-99de-24598c302392",
   "metadata": {},
   "source": [
    "## Key Analytical Use Cases & Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760315e-d333-4e5a-8af8-f44c3767a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "conn_str = os.getenv(\"SALES_DB_URL\")\n",
    "%sql $conn_str\n",
    "%sql --connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e04d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advanced_sql_queries.sql', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2237502",
   "metadata": {},
   "source": [
    "###  1. Profit & Loss by Category, City, and Customer\n",
    "- Business Goal: Track financial performance across dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3250607",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Total profit and amount by product category and city\n",
    "SELECT dp.category, dl.city, SUM(fs.amount) AS total_sales, SUM(fs.profit) AS total_profit\n",
    "FROM olap.fact_sales fs\n",
    "JOIN olap.dim_product dp ON fs.product_sk = dp.product_sk\n",
    "JOIN olap.dim_location dl ON fs.location_sk = dl.location_sk\n",
    "GROUP BY dp.category, dl.city\n",
    "ORDER BY total_profit DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fcf4d",
   "metadata": {},
   "source": [
    "### 2. Order Frequency / Repeat Customers\n",
    "- Business Goal: Measure engagement and loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Top Repeating Customer(s) Only\n",
    "WITH customer_orders AS (\n",
    "    SELECT \n",
    "        dc.customer_name,\n",
    "        COUNT(fs.order_sk) AS order_count,\n",
    "        SUM(fs.amount) AS total_spent,\n",
    "        SUM(fs.profit) AS total_profit\n",
    "    FROM olap.fact_sales fs\n",
    "    JOIN olap.dim_customer dc ON fs.customer_sk = dc.customer_sk\n",
    "    GROUP BY dc.customer_name\n",
    "),\n",
    "ranked_orders AS (\n",
    "    SELECT \n",
    "        customer_name,\n",
    "        order_count,total_spent,total_profit,\n",
    "        RANK() OVER (ORDER BY order_count DESC) AS order_rank\n",
    "    FROM customer_orders\n",
    ")\n",
    "SELECT order_rank, customer_name, total_spent,order_count, total_profit\n",
    "FROM ranked_orders\n",
    "WHERE order_rank = 1\n",
    "ORDER BY total_spent DESC, total_profit DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Rank All Customers by Repeat Frequency(TOP 10)\n",
    "SELECT \n",
    "    dc.customer_name,\n",
    "    COUNT(fs.order_sk) AS order_count,\n",
    "    SUM(fs.amount) AS total_spent,\n",
    "    SUM(fs.profit) AS total_profit,\n",
    "    RANK() OVER (ORDER BY COUNT(fs.order_sk) DESC) AS order_rank\n",
    "FROM olap.fact_sales fs\n",
    "JOIN olap.dim_customer dc ON fs.customer_sk = dc.customer_sk\n",
    "GROUP BY dc.customer_name\n",
    "ORDER BY order_rank, total_spent DESC, total_profit DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba277ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Top 5 Customers by Total Amount Spent        \n",
    "WITH customer_spending AS (\n",
    "    SELECT \n",
    "        dc.customer_name,\n",
    "        SUM(fs.amount) AS total_spent,\n",
    "        COUNT(fs.order_sk) AS order_count\n",
    "    FROM olap.fact_sales fs\n",
    "    JOIN olap.dim_customer dc ON fs.customer_sk = dc.customer_sk\n",
    "    GROUP BY dc.customer_name\n",
    ")\n",
    "SELECT customer_name, total_spent, order_count\n",
    "FROM customer_spending\n",
    "ORDER BY total_spent DESC   \n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0ab00",
   "metadata": {},
   "source": [
    "### 3. Top-Performing Sub-Categories\n",
    "- Business Goal: Identify high-profit niches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4da0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Top sub-categories by average profit per order\n",
    "SELECT dp.sub_category, ROUND(AVG(fs.profit), 2) AS avg_profit\n",
    "FROM olap.fact_sales fs\n",
    "JOIN olap.dim_product dp ON fs.product_sk = dp.product_sk\n",
    "GROUP BY dp.sub_category\n",
    "ORDER BY avg_profit DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f6044",
   "metadata": {},
   "source": [
    "### 4. Peak Sales Months\n",
    "- Business Goal: Inform inventory and marketing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Monthly sales trends\n",
    "SELECT dd.year, dd.month, SUM(fs.amount) AS total_sales\n",
    "FROM olap.fact_sales fs\n",
    "JOIN olap.dim_date dd ON fs.date_id = dd.date_id\n",
    "GROUP BY dd.year, dd.month\n",
    "ORDER BY total_sales DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98553f7",
   "metadata": {},
   "source": [
    "### 5. Cities Above 95th Percentile in Revenue\n",
    "- Business Goal: Prioritize high-potential locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c144907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Identify cities in top 5% of revenue\n",
    "WITH city_sales AS (\n",
    "    SELECT dl.city,dl.state, SUM(fs.amount) AS revenue\n",
    "    FROM olap.fact_sales fs\n",
    "    JOIN olap.dim_location dl ON fs.location_sk = dl.location_sk\n",
    "    GROUP BY dl.city, dl.state\n",
    "),\n",
    "threshold AS (\n",
    "    SELECT PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY revenue) AS cutoff FROM city_sales\n",
    ")\n",
    "SELECT cs.city,cs.state, cs.revenue\n",
    "FROM city_sales cs\n",
    "JOIN threshold t ON cs.revenue > t.cutoff\n",
    "ORDER BY cs.revenue DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53597f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Cumulative Revenue % by City\n",
    "WITH city_sales AS (\n",
    "    SELECT dl.city,dl.state, SUM(fs.amount) AS revenue\n",
    "    FROM olap.fact_sales fs\n",
    "    JOIN olap.dim_location dl ON fs.location_sk = dl.location_sk\n",
    "    GROUP BY dl.city,dl.state\n",
    "),\n",
    "total AS (\n",
    "    SELECT SUM(revenue) AS total_revenue FROM city_sales\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        cs.city,\n",
    "        cs.state,\n",
    "        cs.revenue,\n",
    "        SUM(cs.revenue) OVER (ORDER BY cs.revenue DESC) AS cumulative_revenue,\n",
    "        t.total_revenue\n",
    "    FROM city_sales cs\n",
    "    CROSS JOIN total t\n",
    ")\n",
    "SELECT \n",
    "    city,\n",
    "    state,\n",
    "    revenue,\n",
    "    ROUND((revenue / total_revenue) * 100, 2) AS revenue_pct,\n",
    "    ROUND((cumulative_revenue / total_revenue) * 100, 2) AS cumulative_pct\n",
    "FROM ranked\n",
    " --WHERE (cumulative_revenue / total_revenue) <= 0.80 -- To Get Cities Contributing Up to 80%\n",
    "ORDER BY cumulative_revenue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e7136",
   "metadata": {},
   "source": [
    "### 6. Average Monthly Sales per Product\n",
    "- Business Goal: Track product lifecycle or seasonal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c66d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Monthly average per product\n",
    "SELECT dp.product_sk, dp.sub_category, ROUND(AVG(fs.amount), 2) AS avg_monthly_sales\n",
    "FROM olap.fact_sales fs\n",
    "JOIN olap.dim_product dp ON fs.product_sk = dp.product_sk\n",
    "JOIN olap.dim_date dd ON fs.date_id = dd.date_id\n",
    "GROUP BY dp.product_sk, dp.sub_category\n",
    "ORDER BY avg_monthly_sales DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5941a10",
   "metadata": {},
   "source": [
    "## Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql --close postgresql://postgres:***@localhost:5432/sales_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
