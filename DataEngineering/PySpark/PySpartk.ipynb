{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad6c9af",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION AND CLEANING WITH PYSPARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee6000",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image \n",
    "display(Image(\"spark_architecture1.png\"))\n",
    "display(Image(\"spark_architecture.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb06d33",
   "metadata": {},
   "source": [
    "- **Advantages of Spark :** Spark  is  known  to  handles  large-scale  data  processing  and  it  has  efficient  architecture \n",
    "(Driver, Executors, Cluster Manager).\n",
    ">- Speed: Spark performs up to 100 times faster than MapReduce for processing large amounts of data. It is also able to divide the data into chunks in a controlled way.\n",
    ">- Powerful Caching: Powerful caching and disk persistence capabilities are offered by a simple programming layer.(in memory)\n",
    ">- Deployment: Mesos, Hadoop via YARN, or Spark’s own cluster manager can all be used to deploy it.\n",
    ">- Real-Time: Because of its in-memory processing, it offers real-time computation and low latency.\n",
    ">- Polyglot:  In  addition  to  Java,  Scala,  Python,  and  R,  Spark  also  supports  all  four  of these languages. You can write spark code in any one of these languages. \n",
    ">- Spark also provides a command-line interface in Scala and Python.\n",
    "\n",
    "-  **Cleaning Techniques in Spark**\n",
    ">- Filtering data.\n",
    ">- Aggregations and GroupBy.\n",
    ">- Joining DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c549fee",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506040f",
   "metadata": {},
   "source": [
    "### Case Study Summary: Data Exploration & Cleaning with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd838b3f",
   "metadata": {},
   "source": [
    "**1. Overview**\n",
    "This case study highlights how **Nuga Bank**, a prominent financial institution, adopted **PySpark** to enhance its data exploration and cleaning workflows. The initiative focused on automating data preparation to support better analysis and decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Key Challenges**\n",
    "- Manual, time-consuming data cleaning processes.\n",
    "- Poor scalability with increasing data volume.\n",
    "- Inconsistent data quality impacting analysis.\n",
    "- Difficulty transforming raw data into structured formats.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Objectives**\n",
    "- Implement automated data exploration and cleaning using **PySpark**.\n",
    "- Normalize datasets to **2NF or 3NF** for better integrity.\n",
    "- Load structured data into a **PostgreSQL** database for reporting and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Implementation Steps**\n",
    "\n",
    "- 1. **Data Extraction**\n",
    ">- Setup **Spark Context** for distributed computing.\n",
    ">- Load CSV data into **PySpark DataFrame**.\n",
    "\n",
    "- 2. **Data Transformation**\n",
    ">- Handle missing values, duplicates, and inconsistent formats.\n",
    ">- Normalize data into relational schema.\n",
    "\n",
    "- 3. **Data Loading**\n",
    ">- Store the cleaned and normalized data in a **PostgreSQL Server**.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Benefits**\n",
    ">- **Efficiency**: Reduced manual tasks via automation.\n",
    ">- **Scalability**: Able to process large datasets with ease.\n",
    ">- **Data Quality**: Standardized and consistent data cleaning.\n",
    ">- **Structured Database**: Easier data management and querying.\n",
    ">- **Team Collaboration**: Shared workflows improved communication and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Tech Stack**\n",
    ">- **Programming Languages**: Python, SQL  \n",
    ">- **Processing Framework**: PySpark  \n",
    ">- **Database**: PostgreSQL  \n",
    "\n",
    "---\n",
    "\n",
    "**7. 🔗 Data Source**\n",
    ">- [Google Drive - Dataset](https://drive.google.com/file/d/1WvnxUWIUQcRSXB5so7-PoGPxU9ekSkYb/view?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "> *The project will successfully automate and scale data exploration and cleaning, empowering Nuga Bank with a robust and efficient data foundation for analytics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d5bde",
   "metadata": {},
   "source": [
    "## Case Study Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3fcf2",
   "metadata": {},
   "source": [
    "### Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3024ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "from IPython.display import Image, display\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, monotonically_increasing_id # Great for joining, tracking, or indexing rows when no natural unique key exists.\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from sqlalchemy import (\n",
    "      create_engine, Column, Integer, String, Float, DateTime,\n",
    "      BigInteger, Text, TIMESTAMP, ForeignKey, MetaData, Table\n",
    ")\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe83822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AnnieMona:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NugaBankEtl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x202697b7050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise  Spark Session\n",
    "spark = SparkSession.builder.appName('NugaBankEtl')\\\n",
    ".getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452e70f",
   "metadata": {},
   "source": [
    "### Extraction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CSV content from Google Drive into a temporary file path \n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\") as tmp:\n",
    "    temp_path = tmp.name\n",
    "\n",
    "file_id = \"1WvnxUWIUQcRSXB5so7-PoGPxU9ekSkYb\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", temp_path, quiet=False)\n",
    "\n",
    "shutil.copy(temp_path, \"nuga_bank_transactions.csv\") # Save a permanent copy of the downloaded CSV\n",
    "\n",
    "nuga_bank_df = spark.read.csv(temp_path, header=True, inferSchema=True) # Load into Spark\n",
    "nuga_bank_df.cache()  # Force read and keep in memory\n",
    "nuga_bank_df.count()  # Triggers actual file read\n",
    "\n",
    "\n",
    "os.remove(temp_path) # delete temp file\n",
    "\n",
    "# Show details\n",
    "print(f\"Data Shape: {nuga_bank_df.count()} rows × {len(nuga_bank_df.columns)} columns\")\n",
    "nuga_bank_df.show(5)\n",
    "nuga_bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708d6f9",
   "metadata": {},
   "source": [
    "### Transformation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d70b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuga_bank_df = spark.read.csv(r'nuga_bank_transactions.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daba6f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: 1000000 rows × 23 columns\n",
      "root\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Customer_Address: string (nullable = true)\n",
      " |-- Customer_City: string (nullable = true)\n",
      " |-- Customer_State: string (nullable = true)\n",
      " |-- Customer_Country: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone_Number: string (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = true)\n",
      " |-- IBAN: string (nullable = true)\n",
      " |-- Currency_Code: string (nullable = true)\n",
      " |-- Random_Number: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Is_Active: string (nullable = true)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Shape: {nuga_bank_df.count()} rows × {len(nuga_bank_df.columns)} columns\")\n",
    "nuga_bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f922a1",
   "metadata": {},
   "source": [
    "#### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ac258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "for col in nuga_bank_df.columns:\n",
    "    null_count = nuga_bank_df.filter(nuga_bank_df[col].isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{col}: {null_count} nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23130216",
   "metadata": {},
   "source": [
    "**NULL HANDLING STRATEGY**\n",
    "| Column Category          | Columns                                                                                                                                         | Strategy                                                                                                     |\n",
    "| ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| ✅ Mandatory & Critical   | `Transaction_Date`, `Amount`, `Transaction_Type`                                                                                                | Already non-null — **no action** needed.                                                                     |\n",
    "| 📌 High-Value Optional   | `Currency_Code`, `Category`, `Group`, `Is_Active`, `Random_Number`, `Last_Updated`                                                              | - Use `fillna()` with `\"Unknown\"` or `\"N/A\"` for strings<br>- Use `-1` or a flagged number for numerics      |\n",
    "| Customer Profile      | `Customer_Name`, `Customer_Address`, `Customer_City`, `Customer_State`, `Customer_Country`, `Email`, `Phone_Number`, `Gender`, `Marital_Status` | - Use `\"Unknown\"`  for strings<br>- Optionally drop rows with **≥5 nulls** across these fields |\n",
    "| Financial Identifiers | `Credit_Card_Number`, `IBAN`                                                                                                                    | - Drop these **if not needed downstream** or mask with `\"0000-0000-0000-0000\"` / `\"UNKNOWN\"`                 |\n",
    "| Employment Data    | `Company`, `Job_Title`                                                                                                                          | - Fill missing with `\"Undisclosed\"`                                                        |\n",
    "| Description           | `Description`                                                                                                                                   | - Fill with `\"No description provided\"`  placeholder                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e534ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fill common categorical string fields\n",
    "string_fill = {\n",
    "    \"Customer_Name\": \"Unknown\",\n",
    "    \"Customer_Address\": \"Unknown\",\n",
    "    \"Customer_City\": \"Unknown\",\n",
    "    \"Customer_State\": \"Unknown\",\n",
    "    \"Customer_Country\": \"Unknown\",\n",
    "    \"Email\": \"unknown@example.com\",\n",
    "    \"Phone_Number\": \"Unknown\",\n",
    "    \"Company\": \"Undisclosed\",\n",
    "    \"Job_Title\": \"Unemployed\",\n",
    "    \"Currency_Code\": \"N/A\",\n",
    "    \"Category\": \"N/A\",\n",
    "    \"Group\": \"N/A\",\n",
    "    \"Is_Active\": \"Unknown\",\n",
    "    \"Description\": \"No description provided\",\n",
    "    \"Gender\": \"Unspecified\",\n",
    "    \"Marital_Status\": \"Unspecified\",\n",
    "    \"IBAN\": \"Unknown\"\n",
    "}\n",
    "nuga_bank_df_clean = nuga_bank_df.fillna(string_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c80350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fill numerical fields\n",
    "nuga_bank_df_clean = nuga_bank_df_clean.fillna({\n",
    "    \"Random_Number\": -1.0,\n",
    "    \"Credit_Card_Number\": 0 \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce3f002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fill timestamp\n",
    "nuga_bank_df_clean = nuga_bank_df_clean.fillna({\"Last_Updated\": \"1900-01-01\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b7a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convert all column names to lowercase\n",
    "from pyspark.sql.functions import col\n",
    "nuga_bank_df_clean = nuga_bank_df_clean.select([col(c).alias(c.lower()) for c in nuga_bank_df_clean.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values again\n",
    "for col in nuga_bank_df_clean.columns:\n",
    "    null_count = nuga_bank_df_clean.filter(nuga_bank_df_clean[col].isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{col}: {null_count} nulls\")\n",
    "\n",
    "nuga_bank_df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e080e908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "root\n",
      " |-- transaction_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- customer_name: string (nullable = false)\n",
      " |-- customer_address: string (nullable = false)\n",
      " |-- customer_city: string (nullable = false)\n",
      " |-- customer_state: string (nullable = false)\n",
      " |-- customer_country: string (nullable = false)\n",
      " |-- company: string (nullable = false)\n",
      " |-- job_title: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- phone_number: string (nullable = false)\n",
      " |-- credit_card_number: long (nullable = false)\n",
      " |-- iban: string (nullable = false)\n",
      " |-- currency_code: string (nullable = false)\n",
      " |-- random_number: double (nullable = false)\n",
      " |-- category: string (nullable = false)\n",
      " |-- group: string (nullable = false)\n",
      " |-- is_active: string (nullable = false)\n",
      " |-- last_updated: timestamp (nullable = true)\n",
      " |-- description: string (nullable = false)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- marital_status: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nuga_bank_df_clean.count())\n",
    "nuga_bank_df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba799a",
   "metadata": {},
   "source": [
    "#### Normalise and Split to dim and facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145c1999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = false)\n",
      " |-- customer_name: string (nullable = false)\n",
      " |-- customer_address: string (nullable = false)\n",
      " |-- customer_city: string (nullable = false)\n",
      " |-- customer_state: string (nullable = false)\n",
      " |-- customer_country: string (nullable = false)\n",
      "\n",
      "None\n",
      "+-----------+--------------------+--------------------+------------------+--------------+-------------------+\n",
      "|customer_id|       customer_name|    customer_address|     customer_city|customer_state|   customer_country|\n",
      "+-----------+--------------------+--------------------+------------------+--------------+-------------------+\n",
      "|          0| Dr. Steven Sandoval|71969 Casey Mountain|North Belindaville|     Wisconsin|            Uruguay|\n",
      "|          1|         Jamie Dixon|0146 Veronica Mou...|         Jonesland|      Delaware|       Saint Martin|\n",
      "|          2|         Amber Jones|37115 Peterson Vi...|    West Kellyside|      Delaware|            Unknown|\n",
      "|          3|     Michael Hawkins|             Unknown|         Sarahfort|      Missouri|            Eritrea|\n",
      "|          4|    Joseph Cervantes|59506 Christensen...|      Theresamouth|        Oregon|      French Guiana|\n",
      "|          5|             Unknown|     3758 Randy Fork|       New Michael|      Missouri|              Chile|\n",
      "|          6|      Julie Anderson|81866 Burton Circles|       Swansonview| West Virginia|            Mayotte|\n",
      "|          7|      Christian Wood|162 Jessica Squar...|       North Alvin|       Vermont|             Cyprus|\n",
      "|          8|        Sarah Taylor|445 Evans Lodge A...|          Clarkton|       Unknown|       Turkmenistan|\n",
      "|          9|         Corey Reyes|283 George Cliff ...|  West Dennisville|    California|           Bulgaria|\n",
      "|         10|Christopher Chris...|     585 Avila Creek|      Port William|    New Jersey|              Ghana|\n",
      "|         11|       Alicia Curtis|27812 Cheyenne La...|       Ryanborough|        Alaska|Antigua and Barbuda|\n",
      "|         12|          Kevin Mora|    181 Amanda River|           Unknown| West Virginia|             Greece|\n",
      "|         13|    Amanda Rodriguez|28442 Nicole Cany...|           Unknown|  South Dakota|      Guinea-Bissau|\n",
      "|         14|        Sherry Marks|299 Eric Haven Ap...|    East Katherine|     Louisiana|            Iceland|\n",
      "|         15|Mr. Antonio Zimme...|  0217 Brenda Knolls|        East Karen| Massachusetts|            Ecuador|\n",
      "|         16|        Kevin Jacobs| 91890 Matthew Locks|         Kellerton|       Unknown|            Georgia|\n",
      "|         17|          Mary Green|790 Warren Statio...|       Elliottview|      Maryland|        El Salvador|\n",
      "|         18|       Jordan Coffey|73342 Randolph Mo...|       Triciaville|     Louisiana|            Namibia|\n",
      "|         19|Mrs. Cynthia Smit...|  3650 Miller Common|  New Jenniferland|      Arkansas|              Nepal|\n",
      "+-----------+--------------------+--------------------+------------------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customer = nuga_bank_df_clean \\\n",
    "    .select('customer_name', 'customer_address', 'customer_city', 'customer_state', 'customer_country') \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"customer_id\", monotonically_increasing_id()).cache()\n",
    "\n",
    "\n",
    "dim_customer =dim_customer.select('customer_id','customer_name', 'customer_address', 'customer_city', 'customer_state','customer_country')\n",
    "print(dim_customer.printSchema())\n",
    "dim_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a06c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = false)\n",
      " |-- company: string (nullable = false)\n",
      " |-- job_title: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- phone_number: string (nullable = false)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- marital_status: string (nullable = false)\n",
      "\n",
      "None\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+\n",
      "|employee_id|             company|           job_title|               email|        phone_number|     gender|marital_status|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+\n",
      "|          0|         Undisclosed|Sport and exercis...| unknown@example.com|    001-817-207-5116|       Male|        Single|\n",
      "|          1|       Park and Sons|Race relations of...|   joy43@example.org|          4942627537|     Female|       Married|\n",
      "|          2|       Walker-Hanson|      Hydrogeologist|mendozabrett@exam...|001-617-706-0471x...|       Male|        Single|\n",
      "|          3|        Acosta Group|          Unemployed| unknown@example.com|       (939)448-0792|       Male|        Single|\n",
      "|          4|         Davis-Davis|Engineer, manufac...|  jbaker@example.com|001-609-353-3848x311|       Male|        Single|\n",
      "|          5|Bell, Hogan and M...|      Phytotherapist|johnsonaaron@exam...|          5186355463|       Male|       Married|\n",
      "|          6|       Cortez-Barber|Environmental edu...|   fneal@example.org|    456-712-3026x400|      Other|      Divorced|\n",
      "|          7|   Stevenson-Herrera|Engineer, control...| mbrooks@example.org|   355.290.5208x8977|     Female|        Single|\n",
      "|          8|Taylor, George an...|Public relations ...|schultzalex@examp...|        671-841-3532|Unspecified|   Unspecified|\n",
      "|          9|Keith, Ward and A...|Chief Strategy Of...| unknown@example.com|  348-749-5017x39142|       Male|      Divorced|\n",
      "|         10|    Jimenez-Martinez|   Ceramics designer|lrichard@example.org|    801.552.4366x307|       Male|        Single|\n",
      "|         11|      Mahoney-Coffey|             Barista|michaelking@examp...|  (689)220-1787x9102|     Female|        Single|\n",
      "|         12|      Sanchez-Morris|   Food technologist|macdonaldricky@ex...|             Unknown|       Male|       Married|\n",
      "|         13|       Cunningham-Ho|          Unemployed|barneskimberly@ex...|    767.679.2623x450|Unspecified|   Unspecified|\n",
      "|         14|        Thompson PLC|Aeronautical engi...|edwardmorse@examp...|001-687-795-1557x470|       Male|      Divorced|\n",
      "|         15|    Hansen-Hernandez|       Advice worker|philipgomez@examp...|   448.506.0936x0346|     Female|      Divorced|\n",
      "|         16|Wolf, Zimmerman a...|   Software engineer|richardsonnathan@...|    631-673-4285x621|      Other|        Single|\n",
      "|         17|         Undisclosed|Occupational psyc...|kristenthompson@e...|    001-271-658-0089|Unspecified|      Divorced|\n",
      "|         18|            Hunt PLC|   Librarian, public|   fcook@example.net|             Unknown|       Male|        Single|\n",
      "|         19|Krause, Bell and ...|                 Sub|melissalozano@exa...|    001-274-563-3817|      Other|      Divorced|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_employee = nuga_bank_df_clean \\\n",
    "    .select('company', 'job_title', 'email', 'phone_number','gender', 'marital_status') \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"employee_id\", monotonically_increasing_id()).cache()\n",
    "\n",
    "dim_employee = dim_employee.select('employee_id','company', 'job_title', 'email', 'phone_number','gender', 'marital_status')\n",
    "print(dim_employee.printSchema())\n",
    "dim_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fffca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: long (nullable = false)\n",
      " |-- transaction_date: timestamp (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- description: string (nullable = false)\n",
      "\n",
      "None\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "|transaction_id|    transaction_date|transaction_type|         description|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "|             0|2024-04-11 09:51:...|        Transfer|Determine program...|\n",
      "|             1|2024-03-15 02:27:...|        Transfer|Body this laugh s...|\n",
      "|             2|2024-02-22 06:24:...|         Deposit|Congress more foo...|\n",
      "|             3|2024-01-15 11:54:...|      Withdrawal|Near magazine oil...|\n",
      "|             4|2024-03-20 02:18:...|         Deposit|Fire thing pull g...|\n",
      "|             5|2024-01-10 07:20:...|        Transfer|Dark station offi...|\n",
      "|             6|2024-01-13 10:47:...|        Transfer|No description pr...|\n",
      "|             7|2024-03-25 00:46:...|        Transfer|Speak various so ...|\n",
      "|             8|2024-04-13 05:57:...|        Transfer|Car east differen...|\n",
      "|             9|2024-04-15 20:42:...|         Deposit|Skin perform mout...|\n",
      "|            10|2024-02-24 08:38:...|         Deposit|Gas on meeting ov...|\n",
      "|            11|2024-02-25 21:34:...|         Deposit|Live yourself sur...|\n",
      "|            12|2024-02-13 22:07:...|        Transfer|Girl piece song t...|\n",
      "|            13|2024-03-01 03:55:...|         Deposit|Marriage fill wal...|\n",
      "|            14|2024-01-10 09:32:...|        Transfer|Me myself power. ...|\n",
      "|            15|2024-03-22 01:48:...|        Transfer|Town strong both ...|\n",
      "|            16|2024-04-17 08:09:...|        Transfer|Series president ...|\n",
      "|            17|2024-01-16 08:29:...|      Withdrawal|Tend discussion f...|\n",
      "|            18|2024-04-19 01:08:...|      Withdrawal|Learn available a...|\n",
      "|            19|2024-04-21 10:19:...|      Withdrawal|Expert spend land...|\n",
      "+--------------+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_transaction = nuga_bank_df_clean \\\n",
    "    .select('transaction_date', 'transaction_type','description') \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"transaction_id\", monotonically_increasing_id()).cache()\n",
    "dim_transaction = dim_transaction.select('transaction_id','transaction_date', 'transaction_type','description')\n",
    "print(dim_transaction.printSchema())\n",
    "dim_transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eea61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fact_transaction_sk: long (nullable = false)\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- credit_card_number: long (nullable = false)\n",
      " |-- iban: string (nullable = false)\n",
      " |-- currency_code: string (nullable = false)\n",
      " |-- random_number: double (nullable = false)\n",
      " |-- category: string (nullable = false)\n",
      " |-- group: string (nullable = false)\n",
      " |-- is_active: string (nullable = false)\n",
      " |-- last_updated: timestamp (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------+--------------+-------------+-------------+------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+\n",
      "|fact_transaction_sk|transaction_id|  customer_id|  employee_id|amount| credit_card_number|                iban|currency_code|random_number|category|group|is_active|        last_updated|\n",
      "+-------------------+--------------+-------------+-------------+------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+\n",
      "|                  0|          1544| 901943133749| 816043787761|145.87|   3513174386257189|GB36CCLA699403137...|          LKR|       1048.0|       D|    Y|       No|2022-05-22 10:51:...|\n",
      "|                  1|          4830| 816043790903| 850403529378|858.83|       639049241362|GB03JSTW102425414...|          HNL|       2939.0|       D|    Y|  Unknown|2020-11-18 12:23:...|\n",
      "|                  2|          2362| 738734377266|1348619733293|858.83|                  0|GB15QQFK482310888...|          JMD|       6074.0|       C|    Y|      Yes|2024-03-23 20:14:...|\n",
      "|                  3|           594| 188978561537| 481036337723|479.04|     30481038487779|GB96JGNH751751587...|          NPR|       5169.0|       D|    Y|      Yes|2021-10-09 17:54:...|\n",
      "|                  4|          4156| 867583397814| 360777256958| 904.2|    377380386098918|GB32KETB776604311...|          CNY|       2568.0|     N/A|    X|       No|2020-09-09 23:21:...|\n",
      "|                  5|          4660| 369367191874| 901943136850|707.28|   6541397870571174|GB82VRXY147291545...|          SRD|       1711.0|       D|    X|       No|2022-12-21 00:48:...|\n",
      "|                  6|          4020|1159641173764|1589137903473|664.47|   3560438532195557|GB10VKTW164934247...|          FJD|       5892.0|       A|    Y|      Yes|2022-08-27 14:23:...|\n",
      "|                  7|          1846|1554778162984|1254130452195|412.35|   6537706977931543|GB61VCPY392236616...|          GEL|       4177.0|       B|    X|       No|2021-12-27 19:53:...|\n",
      "|                  8|          2908|1219770714954| 592705489711|842.22|      4496190303947|GB36HMRL558912465...|          BND|       5705.0|       B|    Z|       No|2020-01-06 08:42:...|\n",
      "|                  9|          1678| 197568497241| 807453853282|719.74|     30492564736455|GB62SJTT446379551...|          JOD|       5251.0|       C|    Y|      Yes|2020-10-23 17:05:...|\n",
      "|                 10|          1919|1194000910024|1254130452259|946.24|   4517804150260621|GB15WPME100100288...|          N/A|       2334.0|       A|    Z|      Yes|2023-01-17 21:40:...|\n",
      "|                 11|          4634|1382979473880|1537598296465|548.02|      4230569008427|GB15AIQE663380579...|          SRD|       8683.0|       C|    Y|       No|2021-06-25 09:46:...|\n",
      "|                 12|          2866| 798863919868| 730144443069| 35.48|     30478025177033|GB33VUHP362949078...|          N/A|       4505.0|       A|    Z|      Yes|2022-09-24 23:23:...|\n",
      "|                 13|          4831| 575525622459| 893353202250|987.28|4123950568530992314|GB85SMUF740876309...|          LTL|       7660.0|       A|    X|  Unknown|2022-05-04 03:42:...|\n",
      "|                 14|          3733|1477468753341| 541165882873|378.61|    372967131861585|GB62TYKH895666921...|          N/A|       1941.0|       D|    Y|       No|2022-08-24 11:15:...|\n",
      "|                 15|          3163| 206158433239| 343597386661|303.54|   2720284825542330|GB81GNTD005780301...|          EGP|       2496.0|       B|    Z|      Yes|2022-11-01 18:53:...|\n",
      "|                 16|            42|1529008357419| 377957122079|221.23|       676102320238|GB60OEDH581067525...|          CVE|       8119.0|       A|    Z|  Unknown|2020-12-08 21:09:...|\n",
      "|                 17|          2936| 257698040557| 773094116089| 578.3|      4246079809682|GB85JABH738997597...|          RUB|         -1.0|       B|    Y|       No|2023-12-13 04:16:...|\n",
      "|                 18|          2537| 876173330832| 498216208821|570.73|   4963612162984958|GB98NJRF949463884...|          N/A|       5562.0|       A|    Y|      Yes|2020-09-16 18:44:...|\n",
      "|                 19|          3002| 481036340047| 240518171426|120.14|    180062306519242|GB25ODWZ469624855...|          CUP|       3501.0|       D|    Z|      Yes|2023-03-03 12:28:...|\n",
      "+-------------------+--------------+-------------+-------------+------+-------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build fact_transaction with LEFT JOINs to preserve as many rows as possible\n",
    "fact_transaction = nuga_bank_df_clean \\\n",
    "    .join(dim_customer, on=['customer_name', 'customer_address', 'customer_city','customer_state', 'customer_country'], how='left') \\\n",
    "    .join(dim_employee, on=['company', 'job_title', 'email', 'phone_number','gender', 'marital_status'], how='left') \\\n",
    "    .join(dim_transaction, on=['transaction_date', 'transaction_type', 'description'], how='left') \\\n",
    "    .withColumn(\"fact_transaction_sk\", monotonically_increasing_id()).cache()\\\n",
    "        .select('fact_transaction_sk','transaction_id','customer_id', 'employee_id','amount', 'credit_card_number', 'iban','currency_code', 'random_number',\\\n",
    "        'category', 'group','is_active','last_updated' )\n",
    "print(fact_transaction.printSchema())\n",
    "fact_transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34d06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing customers: 0\n",
      "Missing employees: 0\n",
      "Missing transactions: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify unmatched joins\n",
    "missing_customers = fact_transaction.filter(fact_transaction.employee_id.isNull())\n",
    "print(\"Missing customers:\", missing_customers.count())\n",
    "\n",
    "\n",
    "missing_employees = fact_transaction.filter(fact_transaction.employee_id.isNull())\n",
    "print(\"Missing employees:\", missing_customers.count())\n",
    "\n",
    "missing_transactions = fact_transaction.filter(fact_transaction.transaction_id.isNull())\n",
    "print(\"Missing transactions:\", missing_customers.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81350ab",
   "metadata": {},
   "source": [
    "####  Save the Cleaned Dimension and Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84dba64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet Files (Recommended for DWH / Spark Pipelines)\n",
    "# Parquet is columnar, efficient for query engines, supports schema, and is splittable.\n",
    "\n",
    "dim_customer.write.mode(\"overwrite\").parquet(\"output/dim_customer\")\n",
    "dim_employee.write.mode(\"overwrite\").parquet(\"output/dim_employee\")\n",
    "dim_transaction.write.mode(\"overwrite\").parquet(\"output/dim_transaction\")\n",
    "fact_transaction.write.mode(\"overwrite\").parquet(\"output/fact_transaction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Example: Write to HDFS instead of local\n",
    "dim_customer.write.mode(\"overwrite\").parquet(\"hdfs:///output/dim_customer\")\n",
    "dim_employee.write.mode(\"overwrite\").parquet(\"hdfs:///output/dim_employee\")\n",
    "dim_transaction_ctx.write.mode(\"overwrite\").parquet(\"hdfs:///output/dim_transaction_context\")\n",
    "fact_transaction.write.mode(\"overwrite\").parquet(\"hdfs:///output/fact_transactions\")\n",
    "\n",
    "# output the transformed data as csv\n",
    "transaction.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/transaction')\n",
    "customer.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/customer')\n",
    "employee.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/employee')\n",
    "fact_table.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/fact_table')\n",
    "\n",
    "# Convert to Pandas DataFrames (For Lightweight BI or Local Analysis)\n",
    "# Useful if you want to do quick analysis or load into something like Power BI/Excel.\n",
    "\n",
    "df_customer = dim_customer.toPandas()\n",
    "df_employee = dim_employee.toPandas()\n",
    "dim_transaction_ctx = dim_transaction_ctx.toPandas()\n",
    "fact_transaction = fact_transaction.toPandas()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18621f",
   "metadata": {},
   "source": [
    "### Loading Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfdca90",
   "metadata": {},
   "source": [
    "- Load to a Target Destination (optional based on use-case)\n",
    ">- To a Database: Load into PostgreSQL, BigQuery, Redshift, etc.\n",
    ">- To a Data Lake: Save to cloud storage like AWS S3, Azure Blob, GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e1f4c",
   "metadata": {},
   "source": [
    "####  Load into PostgreSQL or Upload to Cloud Storage (e.g., Amazon S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef874fb0",
   "metadata": {},
   "source": [
    "##### Connect to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51fae997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database engine created successfully.\n",
      "✅ Schema 'olap' created or already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load DB URL and target schema name\n",
    "    db_url = os.getenv(\"NUGA_BANK\")\n",
    "    target_schema = \"olap\"\n",
    "\n",
    "    # Initialize SQLAlchemy engine and psycopg2 connection\n",
    "    engine = create_engine(db_url)\n",
    "    conn = psycopg2.connect(db_url)\n",
    "    conn.autocommit = True\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define metadata with schema\n",
    "    metadata = MetaData(schema=target_schema)\n",
    "\n",
    "    print(\"✅ Database engine created successfully.\")\n",
    "\n",
    "    # Create schema if it does not exist\n",
    "    cursor.execute(\n",
    "        sql.SQL(\"CREATE SCHEMA IF NOT EXISTS {}\").format(sql.Identifier(target_schema))\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Schema '{target_schema}' created or already exists.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to create database engine or schema:\", e)\n",
    "    engine = engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7da2e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = false)\n",
      " |-- customer_name: string (nullable = false)\n",
      " |-- customer_address: string (nullable = false)\n",
      " |-- customer_city: string (nullable = false)\n",
      " |-- customer_state: string (nullable = false)\n",
      " |-- customer_country: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = false)\n",
      " |-- company: string (nullable = false)\n",
      " |-- job_title: string (nullable = false)\n",
      " |-- email: string (nullable = false)\n",
      " |-- phone_number: string (nullable = false)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- marital_status: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: long (nullable = false)\n",
      " |-- transaction_date: timestamp (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- description: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fact_transaction_sk: long (nullable = false)\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- credit_card_number: long (nullable = false)\n",
      " |-- iban: string (nullable = false)\n",
      " |-- currency_code: string (nullable = false)\n",
      " |-- random_number: double (nullable = false)\n",
      " |-- category: string (nullable = false)\n",
      " |-- group: string (nullable = false)\n",
      " |-- is_active: string (nullable = false)\n",
      " |-- last_updated: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(nuga_bank_df_clean.printSchema())\n",
    "display(dim_customer.printSchema())\n",
    "display(dim_employee.printSchema())\n",
    "display(dim_transaction.printSchema())      \n",
    "display(fact_transaction.printSchema())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23ed983",
   "metadata": {},
   "source": [
    "#####   Create Tables with Primary/Foreign Keys/ ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7743e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Probook\\AppData\\Local\\Temp\\ipykernel_17144\\2836978358.py:3: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base(metadata=metadata)\n"
     ]
    }
   ],
   "source": [
    "# --- Base Setup ---\n",
    "metadata = MetaData(schema=target_schema)\n",
    "Base = declarative_base(metadata=metadata)\n",
    "\n",
    "# --- Tables ---\n",
    "\n",
    "# === Customers Table ===\n",
    "class Customer(Base):\n",
    "    __tablename__ = 'customers'\n",
    "\n",
    "    customer_id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    customer_name = Column(Text, nullable=False)\n",
    "    customer_address = Column(Text, nullable=False)\n",
    "    customer_city = Column(Text, nullable=False)\n",
    "    customer_state = Column(Text, nullable=False)\n",
    "    customer_country = Column(Text, nullable=False)\n",
    "\n",
    "    # Reverse relationship\n",
    "    transactions = relationship(\"TransactionFact\", back_populates=\"customer\", cascade=\"all, delete\")\n",
    "\n",
    "\n",
    "# === Employees Table ===\n",
    "class Employee(Base):\n",
    "    __tablename__ = 'employees'\n",
    "\n",
    "    employee_id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    company = Column(Text, nullable=False)\n",
    "    job_title = Column(Text, nullable=False)\n",
    "    email = Column(Text, nullable=False)\n",
    "    phone_number = Column(Text, nullable=False)\n",
    "    gender = Column(Text, nullable=False)\n",
    "    marital_status = Column(Text, nullable=False)\n",
    "\n",
    "    # Reverse relationship\n",
    "    transactions = relationship(\"TransactionFact\", back_populates=\"employee\", cascade=\"all, delete\")\n",
    "\n",
    "\n",
    "# === Transactions Table ===\n",
    "class Transaction(Base):\n",
    "    __tablename__ = 'transactions'\n",
    "\n",
    "    transaction_id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    transaction_date = Column(DateTime, nullable=True)\n",
    "    transaction_type = Column(String(50), nullable=True)\n",
    "    description = Column(Text, nullable=False)\n",
    "\n",
    "    # Reverse relationship\n",
    "    facts = relationship(\"TransactionFact\", back_populates=\"transaction\", cascade=\"all, delete\")\n",
    "\n",
    "\n",
    "# === Transaction Fact Table ===\n",
    "class TransactionFact(Base):\n",
    "    __tablename__ = 'transaction_facts'\n",
    "\n",
    "    fact_transaction_sk = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "\n",
    "    transaction_id = Column(\n",
    "        BigInteger,\n",
    "        ForeignKey(f'{target_schema}.transactions.transaction_id', ondelete='CASCADE'),\n",
    "        primary_key=True\n",
    "    )\n",
    "    customer_id = Column(\n",
    "        BigInteger,\n",
    "        ForeignKey(f'{target_schema}.customers.customer_id', ondelete='CASCADE'),\n",
    "        nullable=False\n",
    "    )\n",
    "    employee_id = Column(\n",
    "        BigInteger,\n",
    "        ForeignKey(f'{target_schema}.employees.employee_id', ondelete='CASCADE'),\n",
    "        nullable=False\n",
    "    )\n",
    "\n",
    "    amount = Column(Float, nullable=True)\n",
    "    credit_card_number = Column(BigInteger, nullable=False)\n",
    "    iban = Column(Text, nullable=False)\n",
    "    currency_code = Column(String(10), nullable=False)\n",
    "    random_number = Column(Float, nullable=False)\n",
    "    category = Column(String(50), nullable=False)\n",
    "    group = Column(String(50), nullable=False)\n",
    "    is_active = Column(String(10), nullable=False)\n",
    "    last_updated = Column(DateTime, nullable=True)\n",
    "\n",
    "    # Relationships (for ORM joins)\n",
    "    customer = relationship(\"Customer\", back_populates=\"transactions\")\n",
    "    employee = relationship(\"Employee\", back_populates=\"transactions\")\n",
    "    transaction = relationship(\"Transaction\", back_populates=\"facts\")\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a520283",
   "metadata": {},
   "source": [
    "##### ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148171b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(\"datamodel.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43bcc4",
   "metadata": {},
   "source": [
    "##### Load Tables : Efficiently load large PySpark DataFrames (from Parquet files) into PostgreSQL tables, using spark.write.jdbc()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f64e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Parquet file\n",
    "customers_df = spark.read.parquet(\"output/dim_customer\")\n",
    "employees_df = spark.read.parquet(\"output/dim_employee\")\n",
    "transaction_df = spark.read.parquet(\"output/dim_transaction\")\n",
    "transaction_fact_df = spark.read.parquet(\"output/fact_transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a2661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define JDBC Settings\n",
    "parsed = urlparse(db_url);\\\n",
    "jdbc_url = f\"jdbc:postgresql://{parsed.hostname}:{parsed.port}{parsed.path}\";\\\n",
    "jdbc_props = {\"user\": parsed.username, \"password\": parsed.password, \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "#print(f\"{jdbc_url }+ {jdbc_props}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcc2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Write Function\n",
    "def write_to_postgres(df, table_name, mode=\"append\"):\n",
    "    try:\n",
    "        print(f\"Writing table: {target_schema}.{table_name} ({df.count()} rows)...\")\n",
    "        df.write \\\n",
    "          .mode(mode) \\\n",
    "          .jdbc(\n",
    "              url=jdbc_url,\n",
    "              table=f\"{target_schema}.{table_name}\",\n",
    "              properties=jdbc_props\n",
    "          )\n",
    "        print(f\"✅ Successfully wrote: {target_schema}.{table_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to write table: {target_schema}.{table_name}\")\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f57983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing table: olap.customers (999884 rows)...\n",
      "✅ Successfully wrote: olap.customers\n",
      "Writing table: olap.employees (999856 rows)...\n",
      "✅ Successfully wrote: olap.employees\n",
      "Writing table: olap.transactions (1000000 rows)...\n",
      "✅ Successfully wrote: olap.transactions\n",
      "Writing table: olap.transaction_facts (1000000 rows)...\n",
      "✅ Successfully wrote: olap.transaction_facts\n"
     ]
    }
   ],
   "source": [
    "# 4. Write All DataFrames to DB\n",
    "write_to_postgres(customers_df, \"customers\", mode=\"append\")        # or \"append\" if incremental\n",
    "write_to_postgres(employees_df, \"employees\", mode=\"append\")\n",
    "write_to_postgres(transaction_df, \"transactions\", mode=\"append\")\n",
    "\n",
    "# Then write facts\n",
    "write_to_postgres(transaction_fact_df, \"transaction_facts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d2cd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop existing session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
