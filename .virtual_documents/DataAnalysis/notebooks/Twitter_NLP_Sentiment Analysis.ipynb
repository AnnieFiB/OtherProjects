














import os
import sys
import importlib
import tweepy
import time
from tweepy import Paginator

sys.path.append("../scripts")
import fetch_tweets as xnlp





# datasets/tweets_labeled.csv
# query = "NNetanyahu lang:en -is:retweet"
df = xnlp.load_tweets()
df.head()








df.columns = ["sentiment", "id", "created_at", "query", "user", "text"]
df.head()


df.info()





# drop irrelevant columns and encode sentiment label if available
tweets_df= df.copy()
tweets_df = xnlp.drop_and_encode_sentiment(tweets_df)
print(tweets_df.info())
tweets_df.head()


# Plotting sentiment distribution to check imbalance .# Works with various column names like 'Sentiment', 'label', etc.
xnlp.plot_sentiment_distribution(tweets_df, title="Sentiment Distribution")


# Plotting tweet trend over time
xnlp.plot_time_trend(tweets_df)


# Plot tweet distribution by season
xnlp.plot_seasonal_trend(tweets_df)





# remove_urls, remove_mentions @mentions, remove_hashtags (bool): Remove hashtags (but keep words if False)
# remove non-alphabetic characters, convert text to lowercase
tweets_df["clean_text"] = tweets_df["text"].apply(xnlp.clean_text)
tweets_df.head()


selected_columns = ["sentiment", "created_at", "clean_text"]
tweets_subset = tweets_df[selected_columns].copy()  
tweets_subset.head()





tweets_subset = xnlp.remove_stopwords(
    tweets_subset,
    text_col="clean_text",
    new_col="filtered_text"  
)
tweets_subset.head()





tweets_subset["filtered_text"] = tweets_subset["filtered_text"].apply(xnlp.lemmatize_text)
tweets_subset.head()








# Top words (Vocabulary insight)
xnlp.plot_top_words(tweets_subset, text_col="filtered_text", top_n=20)





# N-grams: Phrases & context clues
xnlp.plot_ngrams(tweets_subset, text_col="filtered_text", ngram_range=(2, 2), top_n=20)  # bigrams
xnlp.plot_ngrams(tweets_subset, text_col="filtered_text", ngram_range=(3, 3), top_n=20)  # trigrams





# text distribution (by sentiment)
xnlp.plot_wordcloud(tweets_subset, text_col="filtered_text", sentiment="positive")


xnlp.plot_wordcloud(tweets_subset, text_col="filtered_text", sentiment="negative")








# Train,evaluate and select best model 
best_model, vectorizer, results = xnlp.run_training_pipeline(
    df=tweets_subset,
    text_col="filtered_text",         
    label_col="sentiment",           
    max_features=5000               
)





# plot 
xnlp.plot_model_performance(results)


# Display result
print(f"\n Best model '{best_model.name}'  with accuracy: {best_model.score:.4f}")
results





# Save model and vectorizer
print("\n...Saving best model and vectorizer based on accuracy...")
xnlp.save_best_model(best_model, vectorizer)








importlib.reload(xnlp)


















